{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bd0e3f",
   "metadata": {
    "tags": []
   },
   "source": [
    "Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d08b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import snowflake.connector\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import io\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.base import MIMEBase\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import COMMASPACE\n",
    "from email import encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c36cba",
   "metadata": {},
   "source": [
    "Snowflake connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61005614",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = snowflake.connector.connect(user='esteban.correa@RAPPI.COM', \n",
    "                                   authenticator='externalbrowser', \n",
    "                                   account='hg51401', \n",
    "                                   warehouse=\"RP_PERSONALUSER_WH\",\n",
    "                                   database=\"FIVETRAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f606c5f9-7f0a-480d-a4cd-c39dd1dbd6be",
   "metadata": {},
   "source": [
    "Función para SF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d10734",
   "metadata": {},
   "source": [
    "##### SWA, AVL, conversión, ingresos y service level Rappi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691f729",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "swa = \"\"\"with \n",
    "\n",
    "avl_closing as (\n",
    "select final.warehouseid as warehouse_id, final.sync_wh_id, d.external_id as dependencia, final.warehouse_name, final.storereferenceid as storereference_id, final.sync_product_id,\n",
    "ep.plu_padre, ep.plu_hijo, ep.\"FACTOR\", e.ean, final.sales_ayer, final.full_sales_28, final.available_hours, final.total_hours, final.full_available_hours, final.full_total_hours, final.stock_units as stock_rappi\n",
    "from (\n",
    "select a.warehouseid, w.warehouse_id as sync_wh_id, a.warehousename as warehouse_name, a.storereferenceid, p.product_id as sync_product_id, a.sales_units as sales_ayer, a.available_hours, a.total_hours, a.full_available_hours, a.full_total_hours, a.full_sales_28,a.stock_units --a.wl_type, \n",
    "from RP_SILVER_DB_PROD.TURBO_CORE.global_closing_inventory_current as a\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_sync.warehouse_integration as w\n",
    "on w.external_id = a.warehouseid\n",
    "    left join fivetran.co_amysql_turbo_emergency_order_turbo_sync.product_integration as p\n",
    "on p.external_id = a.storereferenceid\n",
    "where a.country_code = 'CO' \n",
    "and a.main_date = current_date - interval '1 day'\n",
    "and p._fivetran_deleted = 'FALSE'\n",
    "and w.integration_name = 'vivo'\n",
    "and p.integration_name = 'vivo') as final\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_sync.warehouse_integration as d\n",
    "on d.warehouse_id = final.sync_wh_id\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_sync.product as e\n",
    "on e.id = final.sync_product_id\n",
    "left join FIVETRAN.CPGS_TURBO_DS_PUBLIC.CO_FILL_RATE_EXITO_PRODUCTO as ep\n",
    "on ep.vivo_id = final.storereferenceid\n",
    "where d.integration_name = 'exito'\n",
    "and ean not in ('1009FLV','1099FLV','10CRAKT23','1128FLV','1142FLV','1150FLV','1154FLV','1164FLV','1207FLV','1218FLV','1244FLV','1257FLV','1288FLV','1293FLV','1313FLV','1375FLV','1418FLV','1432FLV','1461FLV',\n",
    "'1581FLV','1943FLV','1948FLV','24ORCHB23','29ORALM23','LLB1904B','NOEAN1835901','NOEAN1835902','NOEAN1835903','NOEAN1835904','NOEAN1835905', '7,80E+12')),\n",
    "\n",
    "closing_2 as (\n",
    "select a.*, b.vertical, b.atribute, b.product_name, b.macrocategory_name, b.category_name, b.subcategory_name, b.bucket\n",
    "from avl_closing as a\n",
    "left join fivetran.cpgs_turbo_ds_public.global_product_supply as b\n",
    "on a.warehouse_id = b.warehouseid and a.storereference_id = b.storereferenceid\n",
    "having b.wl_type in ('1 Ideal','3 Substitute') \n",
    "and b.active in ('Active' , 'New', 'Inactive')\n",
    "--and atribute = 'No R2E' \n",
    "and product_name != 'Out of catalogo' \n",
    "and b.vertical = 'Turbo'\n",
    "and b.country_code = 'CO'),\n",
    "\n",
    "wl as (\n",
    "select city, warehouseid, storereferenceid, scope, wl_type\n",
    "from fivetran.cpgs_turbo_ds_public.global_wishlist_with_pareto_new\n",
    "where country_code = 'CO'),\n",
    "\n",
    "closing_3 as (\n",
    "select *, case when full_sales_28 < 0 then 0 else full_sales_28 end as vt, w.city as city_name, w.scope as scope_wh, w.wl_type as is_wl\n",
    "from closing_2 as c\n",
    "left join wl as w\n",
    "on w.warehouseid = c.warehouse_id and w.storereferenceid = c.storereference_id),\n",
    "\n",
    "recepcion as (\n",
    "select cast(last_value(k.created_at) over (partition by l.warehouse_id, l.product_id order by k.created_at) as date) as reception_date,\n",
    "l.warehouse_id, l.product_id, k.lot_stock\n",
    "from fivetran.co_amysql_turbo_emergency_order_turbo_inventory_savvy_ms.kardex as k\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_inventory_savvy_ms.lot as l \n",
    "on k.lot_id = l.id\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_inventory_savvy_ms.kardex_type as kt\n",
    "on kt.id = k.kardex_type_id \n",
    "where k.kardex_type_id in (1,19)),\n",
    "\n",
    "transformation as (\n",
    "select convert_timezone('America/Bogota', transformation_date) as transformation_date, warehouse_id, product_id, sum(transfor_units) as transfor_units\n",
    "from (\n",
    "select last_value(k.created_at) over (partition by l.warehouse_id, l.product_id order by k.created_at) as transformation_date,\n",
    "l.warehouse_id, l.product_id, k.lot_stock as transfor_units\n",
    "from fivetran.co_amysql_turbo_emergency_order_turbo_inventory_savvy_ms.kardex as k\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_inventory_savvy_ms.lot as l \n",
    "on k.lot_id = l.id\n",
    "left join fivetran.co_amysql_turbo_emergency_order_turbo_inventory_savvy_ms.kardex_type as kt\n",
    "on kt.id = k.kardex_type_id \n",
    "where k.kardex_type_id in (5)\n",
    "and k.detail = 'transformation_in_unit')\n",
    "group by transformation_date, warehouse_id, product_id),\n",
    "\n",
    "ns as (\n",
    "select q1.WAREHOUSE_ID_NEW, q1.product_id, q1.planeado as planned_3, q1.entregado as delivered_3, \n",
    "(delivered_3 / nullif(planned_3,0)) as service_level_3, q2.planeado_2 as planned_lw, q2.entregado_2 as delivered_lw, \n",
    "(delivered_lw / nullif(planned_lw,0)) as service_level_lw\n",
    "from (\n",
    "select WAREHOUSE_ID_NEW, product_id, sum(planned) as planeado, sum(delivered) as entregado\n",
    "from fivetran.cpgs_turbo_ds_public.global_po_main_current\n",
    "where commited_date >= current_date() - interval '3 day'\n",
    "and country_code = 'CO'\n",
    "and status <> 'CANCELED'\n",
    "group by WAREHOUSE_ID_NEW, product_id) as q1\n",
    "inner join (\n",
    "select WAREHOUSE_ID_NEW, product_id, sum(planned) as planeado_2, sum(delivered) as entregado_2\n",
    "from fivetran.cpgs_turbo_ds_public.global_po_main_current\n",
    "where commited_date between date_trunc('w', current_date - interval '7 days') and date_trunc('w', current_date) - interval '1 days'\n",
    "and country_code = 'CO'\n",
    "and status <> 'CANCELED'\n",
    "group by WAREHOUSE_ID_NEW, product_id) as q2\n",
    "on q1.WAREHOUSE_ID_NEW = q2.WAREHOUSE_ID_NEW and q1.product_id = q2.product_id),\n",
    "\n",
    "top_80 as (\n",
    "select storereferenceid, wl_name, cat as performance_catman\n",
    "from fivetran.cpgs_turbo_ds_public.global_top_80\n",
    "where country_code = 'CO'\n",
    "and cat in ('4.-DEAD', '3.-UGLY_4')\n",
    "and first_sale>60),\n",
    "\n",
    "sacar_wl as (\n",
    "select warehouseid, storereferenceid, keep_in_wl\n",
    "from fivetran.cpgs_turbo_ds_public.co_portfolio_out_review \n",
    "where product_new = 'FALSE'),\n",
    "\n",
    "merma as (\n",
    "select lw.warehouseid as sync_wh_id, round(lw.storereferenceid,0) as sync_product_id,\n",
    "shrinkage_usd_cw, shrinkage_units_cw,\n",
    "round(sum(case when lw.type = 'KNOWN' then lw.usd_amount else 0 end),2) as shrinkage_usd_lw,\n",
    "round(sum(case when lw.type = 'KNOWN' then lw.units else 0 end),2) as shrinkage_units_lw\n",
    "from fivetran.cpgs_turbo_ds_public.global_finance_shrinkage as lw\n",
    "left join (\n",
    "select warehouseid as sync_wh_id, round(storereferenceid,0) as sync_product_id, \n",
    "round(sum(case when type = 'KNOWN' then usd_amount else 0 end),2) as shrinkage_usd_cw,\n",
    "round(sum(case when type = 'KNOWN' then units else 0 end),2) as shrinkage_units_cw\n",
    "from fivetran.cpgs_turbo_ds_public.global_finance_shrinkage\n",
    "where country_code = 'CO'\n",
    "and main_date >= date_trunc('w',current_date)\n",
    "group by sync_wh_id, sync_product_id\n",
    ") as cw\n",
    "on lw.warehouseid = cw.sync_wh_id and round(lw.storereferenceid,0) = cw.sync_product_id\n",
    "where lw.country_code = 'CO'\n",
    "and lw.main_date between date_trunc('w',current_date) - interval '7 day' and date_trunc('w',current_date) - interval '1 day'\n",
    "group by lw.warehouseid, lw.storereferenceid, shrinkage_usd_cw, shrinkage_units_cw),\n",
    "\n",
    "forecast as (\n",
    "select warehouse_id as sync_wh_id, product_id as sync_product_id, sum(forecast) as forecast_lw, sum(sales_units) as sales_lw\n",
    "from RP_SILVER_DB_PROD.TURBO_CORE.GLOBAL_FORECAST_MAIN\n",
    "where country = 'CO'\n",
    "and date between date_trunc('week', current_date - interval '7 day') and date_trunc('week', current_date) - interval '1 day'\n",
    "group by sync_wh_id, sync_product_id),\n",
    "\n",
    "sales28 as (\n",
    "select warehouse_id as sync_wh_id, product_id as sync_product_id, sum(forecast) as forecast_lw, sum(sales_units) as sales_28\n",
    "from RP_SILVER_DB_PROD.TURBO_CORE.GLOBAL_FORECAST_MAIN\n",
    "where country = 'CO'\n",
    "and date between date_trunc('week', current_date - interval '28 day') and date_trunc('week', current_date) - interval '1 day'\n",
    "group by sync_wh_id, sync_product_id)\n",
    "\n",
    "\n",
    "select c.city_name, c.warehouse_id, c.sync_wh_id, c.dependencia, c.warehouse_name, c.storereference_id, c.sync_product_id, c.plu_padre, c.plu_hijo, c.\"FACTOR\", \n",
    "cast(c.ean as numeric) as ean, c.is_wl, c.scope_wh, c.product_name, c.macrocategory_name, c.category_name, c.subcategory_name, c.bucket, c.stock_rappi,\n",
    "c.sales_ayer, f.forecast_lw, f.sales_lw, round(div0(sum(c.full_available_hours),sum(c.full_total_hours))*100,2) as avl_nacional, r.reception_date, \n",
    "to_varchar(t.transformation_date, 'YYYY-MM-DD HH24:MI:SS') as transformation_date, t.transfor_units, zeroifnull(n.planned_3) as planned_3, \n",
    "zeroifnull(n.delivered_3) as delivered_3, zeroifnull(n.service_level_3) as service_level_3, zeroifnull(n.planned_lw) as planned_lw, \n",
    "zeroifnull(n.delivered_lw) as delivered_lw, zeroifnull(service_level_lw) as service_level_lw, p.performance_catman, \n",
    "case when s.keep_in_wl = 'Se recomienda sacar de la WL' then 1 else 0 end as remove_wl, zeroifnull(shrinkage_usd_cw) as shrinkage_usd_cw, \n",
    "zeroifnull(shrinkage_units_cw) as shrinkage_units_cw, zeroifnull(shrinkage_usd_lw) as shrinkage_usd_lw, \n",
    "zeroifnull(shrinkage_units_lw) as shrinkage_units_lw\n",
    "from closing_3 as c\n",
    "left join recepcion as r\n",
    "on r.warehouse_id = c.sync_wh_id and r.product_id = c.sync_product_id\n",
    "left join transformation as t\n",
    "on t.warehouse_id = c.sync_wh_id and t.product_id = c.sync_product_id\n",
    "left join ns as n\n",
    "on n.WAREHOUSE_ID_NEW = c.warehouse_id and n.product_id = c.storereference_id\n",
    "left join top_80 as p\n",
    "on p.wl_name = c.scope_wh and p.storereferenceid = c.storereference_id\n",
    "left join sacar_wl as s \n",
    "on s.warehouseid = c.warehouse_id and s.storereferenceid = c.storereference_id\n",
    "left join merma as m\n",
    "on m.sync_wh_id = c.sync_wh_id and m.sync_product_id = c.sync_product_id\n",
    "left join forecast as f\n",
    "on f.sync_wh_id = c.sync_wh_id and f.sync_product_id = c.sync_product_id\n",
    "left join sales28 as s28\n",
    "on s28.sync_wh_id = c.sync_wh_id and s28.sync_product_id = c.sync_product_id\n",
    "group by c.city_name, c.warehouse_id, c.sync_wh_id, c.dependencia, c.warehouse_name, c.storereference_id, c.sync_product_id, c.plu_padre, c.plu_hijo, c.\"FACTOR\", \n",
    "c.ean, c.is_wl, c.scope_wh, c.product_name, c.macrocategory_name, c.category_name, c.subcategory_name, c.bucket, c.stock_rappi, c.sales_ayer, f.forecast_lw,\n",
    "f.sales_lw, s28.sales_28,  c.vt, c.full_available_hours, c.full_total_hours, r.reception_date, t.transformation_date, \n",
    "t.transfor_units, n.planned_3, n.delivered_3, service_level_3, n.planned_lw, n.delivered_lw, service_level_lw, p.performance_catman, s.keep_in_wl, \n",
    "shrinkage_usd_cw, shrinkage_units_cw, shrinkage_usd_lw, shrinkage_units_lw \n",
    "order by f.forecast_lw desc;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4330ca26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_swa = pd.read_sql(swa,conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d6613a-b470-4892-b46b-82a741327231",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convertir_nombres_a_minusculas(df):\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    return df\n",
    "\n",
    "df_swa = convertir_nombres_a_minusculas(df_swa)\n",
    "\n",
    "df_swa.head()\n",
    "\n",
    "df_swa.loc[df_swa['plu_padre'] == '255010'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d50b60",
   "metadata": {},
   "source": [
    "##### Archivo éxito"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f15393",
   "metadata": {},
   "outputs": [],
   "source": [
    "exito = pd.read_excel(r\"C:\\Users\\juane\\Downloads\\base_exito.xlsx\")\n",
    "exito.loc[exito['Plu PluCD'] == 255010].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334bbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_no_eliminar = ['Dependencia DependenciaCD', 'Plu PluCD', 'Proveedor NombreProveedor', 'Proveedor Nit', \n",
    "                        'Descripcionestadoplu ID', 'Cediatiende ID', 'Inventario', 'PedidoPendiente', 'StockDePresentacion', \n",
    "                        'InventarioMaximo','Descripcionestadoplu ID', 'Umd']\n",
    "\n",
    "exito.drop(columns=[col for col in exito if col not in columnas_no_eliminar], inplace=True)\n",
    "exito.head(3)\n",
    "exito.loc[exito['Plu PluCD'] == 255010].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f43a45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exito.rename(columns={'Dependencia DependenciaCD':'dependencia', 'Plu PluCD':'plu_padre', \n",
    "                      'Proveedor NombreProveedor':'supplier_name', 'Proveedor Nit':'nit_supplier',\n",
    "                      'Descripcionestadoplu ID':'plu_status', 'Cediatiende ID':'cedi', 'Inventario':'stock_exito', \n",
    "                      'PedidoPendiente':'pp', 'StockDePresentacion':'sp', 'InventarioMaximo':'max_inventory', \n",
    "                      'Umd':'umd'}, inplace=True)\n",
    "##exito.head(2)\n",
    "exito.loc[exito['plu_padre'] == 255010].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b240275",
   "metadata": {},
   "source": [
    "##### Unión información"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e1ae25-bcff-413e-9ead-bd7d2299a5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Limpiar 'plu_padre' en ambos DataFrames y convertir a número\n",
    "df_swa['plu_padre'] = df_swa['plu_padre'].astype(str).str.replace('.0', '', regex=False)\n",
    "df_swa['plu_padre'] = pd.to_numeric(df_swa['plu_padre'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "exito['plu_padre'] = exito['plu_padre'].astype(str).str.replace('.0', '', regex=False)\n",
    "exito['plu_padre'] = pd.to_numeric(exito['plu_padre'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Limpiar 'dependencia' en ambos DataFrames y convertir a número\n",
    "df_swa['dependencia'] = df_swa['dependencia'].astype(str).str.replace('.0', '', regex=False)\n",
    "df_swa['dependencia'] = pd.to_numeric(df_swa['dependencia'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "exito['dependencia'] = exito['dependencia'].astype(str).str.replace('.0', '', regex=False)\n",
    "exito['dependencia'] = pd.to_numeric(exito['dependencia'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "#print(df_swa[['dependencia', 'plu_padre']].head())\n",
    "#print(exito[['dependencia', 'plu_padre']].head())\n",
    "exito.loc[exito['plu_padre'] == 255010].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52090e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir 'plu_padre' y 'dependencia' en números en ambos DataFrames\n",
    "df_swa['plu_padre'] = pd.to_numeric(df_swa['plu_padre'], errors='coerce').fillna(0).astype(int)\n",
    "exito['plu_padre'] = pd.to_numeric(exito['plu_padre'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "df_swa['dependencia'] = pd.to_numeric(df_swa['dependencia'], errors='coerce').fillna(0).astype(int)\n",
    "exito['dependencia'] = pd.to_numeric(exito['dependencia'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Realizar el merge asegurando que ambas columnas son del mismo tipo\n",
    "swa_exito = pd.merge(left=df_swa, right=exito, how='left', on=['dependencia', 'plu_padre'])\n",
    "\n",
    "# Mostrar las primeras 10 filas\n",
    "swa_exito.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcba1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "swa_exito = swa_exito.loc[:,['city_name', 'warehouse_id', 'sync_wh_id', 'dependencia', 'warehouse_name', 'storereference_id', \n",
    "                  'sync_product_id', 'plu_padre', 'plu_hijo', 'factor', 'ean', 'is_wl', 'scope_wh', 'product_name',\n",
    "                  'macrocategory_name', 'category_name', 'subcategory_name', 'bucket', 'stock_rappi', 'stock_exito', \n",
    "                  'forecast_lw', 'sales_lw','sales_ayer', 'avl_nacional',\n",
    "                  'reception_date', 'transfor_units', 'planned_3', 'delivered_3', 'service_level_3', \n",
    "                  'planned_lw', 'delivered_lw',\n",
    "                  'shrinkage_usd_cw', 'shrinkage_units_cw', 'shrinkage_usd_lw', 'shrinkage_units_lw', 'supplier_name', \n",
    "                  'plu_status', 'cedi', 'pp', 'sp', 'max_inventory', 'umd']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f54fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verificar la forma del DataFrame resultante antes de eliminar duplicados\n",
    "print(\"Dimensiones del DataFrame antes de eliminar duplicados:\", swa_exito.shape)\n",
    "\n",
    "# Convertir las columnas a los tipos de datos deseados\n",
    "swa_exito_unique = swa_exito.astype({'dependencia':'float','avl_nacional':'float',\n",
    "                                      'planned_3':'int','delivered_3':'int','service_level_3':'float',\n",
    "                                      'planned_lw':'int','delivered_lw':'int'})\n",
    "\n",
    "# Eliminar duplicados basados en 'warehouse_id' y 'sync_wh_id'\n",
    "swa_exito_unique = swa_exito.drop_duplicates(subset=['warehouse_id', 'sync_product_id'])\n",
    "\n",
    "# Verificar la forma del DataFrame resultante después de eliminar duplicados\n",
    "print(\"Dimensiones del DataFrame después de eliminar duplicados:\", swa_exito_unique.shape)\n",
    "\n",
    "# Mostrar los primeros 8 registros del DataFrame resultante\n",
    "print(swa_exito_unique.head(8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fac2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "hoy = datetime.today().strftime('%Y-%m-%d')\n",
    "\n",
    "swa_exito_unique.to_excel(f'Control_tower_supply_{hoy}.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c9037",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace with the path to your service account JSON file\n",
    "creds = Credentials.from_service_account_file('optimal-torch-439314-q9-2c512ed2a125.json')\n",
    "\n",
    "# Replace with the name of the folder where you want to upload the file\n",
    "folder_name = 'ControlTower'\n",
    "\n",
    "# Replace with the path to the Excel file you want to upload\n",
    "file_path = f'Control_tower_supply_{hoy}.xlsx'\n",
    "\n",
    "# Create a Drive API client\n",
    "drive_service = build('drive', 'v3', credentials=creds)\n",
    "\n",
    "# Find the ID of the target folder\n",
    "query = \"mimeType='application/vnd.google-apps.folder' and trashed=false and name='{}'\".format(folder_name)\n",
    "response = drive_service.files().list(q=query, fields='files(id)').execute()\n",
    "folder_id = response.get('files', [])[0].get('id')\n",
    "\n",
    "# Upload the Excel file to Drive\n",
    "file_metadata = {'name': f\"Control_tower_supply_{hoy}.xlsx\", 'parents': [folder_id]}\n",
    "media = MediaFileUpload(file_path, resumable=True)\n",
    "file = drive_service.files().create(body=file_metadata, media_body=media, fields='id').execute()\n",
    "\n",
    "# Give people edit access to the file\n",
    "file_id = file.get('id')\n",
    "\n",
    "\n",
    "new_permissions = [\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'johan.mayorga@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'marcel.kempe@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'julian.ariza@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'roland.rojas@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'antonio.fernandez@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'santiago.lozano@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'mariangela.uribe@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'santiago.martin@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'Helena.kuntz@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'juliana.zarate@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'andrea.cabrales@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'jose.russi@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'esteban.correa@rappi.com'},\n",
    "    {'type': 'user', 'role': 'writer', 'emailAddress': 'maria.mosquera@rappi.com'}\n",
    "]\n",
    "\n",
    "for permission in new_permissions:\n",
    "    drive_service.permissions().create(\n",
    "        fileId=file_id,\n",
    "        body={\n",
    "            'kind': 'drive#permission',\n",
    "            'type': 'user',\n",
    "            'role': permission['role'],\n",
    "            'emailAddress': permission['emailAddress']\n",
    "        },\n",
    "        sendNotificationEmail=True  # optional\n",
    "    ).execute()\n",
    "\n",
    "# Print the link to the file\n",
    "link = \"https://drive.google.com/file/d/{}/view?usp=sharing\".format(file_id)\n",
    "print(\"File uploaded to Drive and shared with the specified email addresses:\")\n",
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5fb8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configura las credenciales de tu cuenta de correo electrónico\n",
    "sender_email = 'esteban.correa@rappi.com'\n",
    "sender_password = 'yexw ulvz yakj ggpc'\n",
    "remitente = \"\"\"Rappi Turbo Supply\"\"\"\n",
    "\n",
    "\n",
    "recipient_emails = ['juliana.zarate@rappi.com','naidys.yepes@rappi.com','santiago.martin@rappi.com', \n",
    "                    'mariangela.uribe@rappi.com','marcel.kempe@rappi.com',\n",
    "                    'johan.mayorga@rappi.com','julian.ariza@rappi.com','antonio.fernandez@rappi.com',\n",
    "                    'santiago.lozano@rappi.com', 'roland.rojas@rappi.com', 'Helena.kuntz@rappi.com','andrea.cabrales@rappi.com', 'jose.russi@rappi.com','esteban.correa@rappi.com','maria.mosquera@rappi.com']\n",
    "\n",
    "# Construye el mensaje de correo electrónico en formato HTML\n",
    "subject = f'Control_tower_supply_{hoy}.xlsx'\n",
    "body = f\"\"\"Hola,<br><br>Adjunto encontrarán el <a href=\"{link}\">INFORME</a> de agotados, SWA y alertas.<br><br>¡Gracias!\"\"\"\n",
    "\n",
    "# Envía el correo electrónico a cada destinatario en la lista\n",
    "for recipient_email in recipient_emails:\n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = remitente\n",
    "    message['To'] = recipient_email\n",
    "    message['Subject'] = subject\n",
    "    message.attach(MIMEText(body, 'html'))\n",
    "\n",
    "    # Establece la conexión con el servidor SMTP de Gmail\n",
    "    try:\n",
    "        server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "        server.starttls()\n",
    "        server.login(sender_email, sender_password)\n",
    "\n",
    "        # Envía el mensaje de correo electrónico\n",
    "        server.sendmail(sender_email, recipient_email, message.as_string())\n",
    "        print(f'Correo electrónico enviado con éxito a {recipient_email}')\n",
    "    except Exception as e:\n",
    "        print(f'Error al enviar el correo electrónico a {recipient_email}: {str(e)}')\n",
    "    finally:\n",
    "        # Cierra la conexión con el servidor SMTP\n",
    "        server.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7c690-e405-43f0-84ac-2fc77e2126d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
